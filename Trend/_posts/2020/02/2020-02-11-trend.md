---
layout: post
title: Medium - Understanding Neural Networks
description: >
  <a href="https://towardsdatascience.com/understanding-neural-networks-19020b758230"> 원문 - Tony Yiu </a>
author: author
comments: true
---

Trend 파악을 위한 Medium 기고문 포스팅 - 뉴럴 네트워크에 대한 이해; 뉴럴 네트워크가 어떻게 동작하는지 알아보면서 딥러닝에 대해 알아봅시다.

<center>
<img src="https://miro.medium.com/max/2999/1*7wN5t9ILU0fpnhbMX2vtng.jpeg"/>
</center>

딥러닝은 요즘 아주 핫한 주제입니다. 그렇지만 무엇이 다른 머신러닝에 비해서 그렇게 특별한 것일까요? 이것은 꽤나 심도있는 질문이기 때문에 먼저 뉴럴 네트워크의 기본에 대해 배워봅시다. 인공신경망 네트워크는 딥러닝의 작업의 핵심입니다. 블랙박스 처럼 보이지만 실제로 깊게 파고 들어가보면 다른 모델처럼 예측을 달성하기 위한 것이죠. 이 포스트에서는 인공신경망 네트워크에 대해서 알아보고 결과적으로 어떻게 동작하는지, 뭘 하는건지 직관적으로 이해할 수 있게 되면 좋겠습니다.

## The 30,000 Feet View

먼저 우리가 다룰 수 있는 최상위 레벨에서 시작합니다. ** 신경 네트워크는 다수의 뉴런 네트워크로 우리가 예측을 하는데 사용하는 것입니다. ** 아래의 다어이그램은 5개의 입력을 받고 5개의 출력을 내뱉는 2개의 히든 레이어를 가진 신경 네트워크 입니다.

<center>
<img src="https://miro.medium.com/max/796/1*yGMk1GSKKbyKr_cMarlWnA.jpeg"/>
</center>
Neural network with two hidden layers
{:.figure}

먼저 왼쪽에서 부터 설명을 드리겠습니다.

1. 우리 모델의 입력값은 오랜지 색입니다.
1. 뉴런의 첫번째 히든 레이어는 파란색입니다.
1. 뉴런의 두번째 히든 레이어는 자주색입니다.
1. 우리 모델의 출력 결과는 초록색입니다.

점들을 잇는 선은 모든 뉴런이 어떻게 내부적으로 연결되어 있고 입력 레이어로부터 출력 레이어까지 어떤 경로를 거치는지 보여줍니다. 나중에 우리는 각 결과값에 대해서 단계별로 계산을 해볼 것입니다. 또한 신경 네트워크에서 회고로 알려진 실수에서 배우는 것을 어떻게 하는지 알아볼 것입니다.

## Getting our Bearings

먼저 우리의 방향성부터 확실히 합시다. 신경 네트워크는 정확히 뭘 하려는 것일까요? 다른 모델처럼 정확한 예측을 하려고 하는 것입니다. 입력값들과 짝을 이루는 결과 값들이 주어진다면 우리는 이러한 데이터 셋을 이용해서 입력값을 받으면 최대한 비슷한 결과값을 뱉어주려고 하는 것입니다.

위에서 그렸던 복잡한 그림은 잊어버리시고 지금은 아래의 간단한 신경 네트워크를 봅시다.

<center>
<img src="https://miro.medium.com/max/701/1*aXK8cx57gGpTSStPSv5kqw.jpeg"/>
</center>
Logistic regression (with only one feature) implemented via a neural network
{:.figure}

이것은 신경 네트워크로 단일 회고 논리를 표현한 것입니다. 회고 논리 방정식을 다시 정의하고 싶다면 위의 그림에서 보였던 네트워크 요소와 방정식 요소의 색깔을 비교하시면 됩니다.

<center>
<img src="https://miro.medium.com/max/701/1*pbtFUSaW7UKrGgg-jbARxw.jpeg"/>
</center>
Logistic regression equation
{:.figure}

각 요소를 알아봅시다.

1. X (오렌지색) 는 우리의 단일 입력값으로 모델의 예측을 위해 주어집니다.
1. B1 (청록색)은 우리의 단일 회고 논리에서 예측에 사용될 매개변수 입니다. B1은 입력값 X와 히든 레이어 1에 있는 파란색 뉴런과 연결하는 선이라는 것이 중요합니다.
1. B0는 바이어스로 리그레션에서 인터셉트 텀과 매우 유사합니다. 신경 네트워크에서의 차이점은 모든 뉴런은 자신의 바이어스가 있다는 것이죠. 리그레션에서 모델은 오직 하나의 인터셉트 텀을 가집니다.
1. 파란색 뉴런은 sigmoid 활성 함수를 포함하고 있습니다 ( 파란색 내부의 곡선을 가리킵니다. ) sigmoid 함수는 log-odds에서 probability로 가는데 사용됩니다.
1. 그리고 `(B1*X+B0)`의 값에 sigmoid함수를 적용하면 예측 값을 얻게됩니다.

그리 나쁘진 않죠? 좀 더 개량해봅시다. 초간단 신경 네트워크는 다음과 같은 컴포넌트들로 구성됩니다.
* 연결 ( 예제에는 여러개의 커넥션이 있으며 각각에는 뉴런으로 연결될 때 가중치가 있습니다. ) 내부에는 가중치가 있으며 B1(입력) 을 변환하여 뉴런으로 전달합니다.
* 뉴런은 B0 바이어스를 가지고 있으며 활성 함수인 sigmoid를 가지고 있습니다.

** 그리고 이 두 객체들이 뉴런 네트워크를 구성하는 기본 요소입니다 ** 더욱 복잡한 신경 네트워크들이 많지만 단지 히든 레이어들이 더 많거나 (뉴런이 많음) 뉴런 사이의 연결이 많은 정도 입니다. 그리고 연결망이 더욱 복잡해지는 것이 우리가 뉴런 네트워크가 학습한다고 일컫는 것이며 우리의 입력-결과 값과 히든 레이어들 간의 연결을 말하는 것입니다.

## Let's Add a Bit of Complexity Now

이제 기본 프레임워크는 알게 되었으니 다시 살짝 복잡한 뉴런 네트워크로 돌아갑시다. 그리고 입력에서 결과값으로 가는 것을 살펴봅시다.

<center>
<img src="https://miro.medium.com/max/796/1*yGMk1GSKKbyKr_cMarlWnA.jpeg"/>
</center>
Our slightly more complicated neural network
{:.figure}

첫번째 히든 레이어는 두개의 뉴런으로 구성되어 있습니다. 그래서 5개의 입력들은 첫번째 히든레이어의 뉴런에게 연결되어야 하고 총 10개의 연결이 생깁니다. 다음 이미지는 입력1과 히든 레이어1과의 연결을 보여줍니다.

<center>
<img src="https://miro.medium.com/max/418/1*QKImlDHkRV-KkciOHxn-dw.jpeg"/>
</center>
The connections between Input 1 and Hidden Layer 1
{:.figure}

주목하셔야 할점은 연결에 가중치가 있다는 것입니다. `W1,1`은 입력 1과 뉴런 1 사이의 연결을 의미하며 `W1,2`는 입력 1과 뉴런2의 입력을 말합니다. 따라서 `Wa,b`는 입력 a에서 뉴런 b사이의 연결이 됩니다. 이제 히든 레이어에 있는 각 뉴런들의 결과를 계산합시다. 여기에서는 다음과 같은 공식을 사용할 것입니다.

`Z1 = W1*In1 + W2*In2 + W3*In3 + W4*In4 + W5*In5 + Bias_Neuron1`

`Neuron 1 Activation = Sigmoid(Z1)`

이러한 계산을 하기 위해 다음과 같이 행렬을 쓸 것입니다.

<center>
<img src="https://miro.medium.com/max/703/1*VxKto8Z35gqWFLFcf0wQ4g.jpeg"/>
</center>
Matrix math makes our life easier
{:.figure}

이전 계층이 m개의 요소이고 현재 계층이 n개의 요소일 때 뉴럴네트워크는 다음과 같이 일반화 할 수 있습니다.

`[W] @ [X] + [Bias] = [Z]`
[W]는 가중치를 나타내는 NxM 행렬이고 [X]는 Mx1 행렬로 입력이나 이전 레이어의 액티베이션에서 가져오는 것이며 [Bias]는 Nx1 행렬로 뉴런 바이어스며 [Z]는 Nx1 행렬로 중간 결과값입니다. 위의 공식에서는 파이썬 표기법을 따라서 행렬 곱셈에 앳(@)표시를 사용했습니다. 우리가 [Z]를 얻어내면 각 요소를 활성화 함수에 적용할 수 있으며 현재 레이어의 결과값을 얻어낼 수 있습니다.

마지막으로 우리가 다음으로 가기전에 이런 요소들을 시각화 하겠습니다.

<center>
<img src="https://miro.medium.com/max/466/1*o3KBHNQsEXsYm0umpZiALg.jpeg"/>
</center>
Visualizing [W], [X], and [Z]
{:.figure}

반복적으로 [Z]를 계산하고 활성 함수에 적용을 함으로써 우리는 입력에서 결과값으로 갈 수 있습니다. 이런 프로세스가 전방 전파입니다. 이제 결과값이 어떻게 계산되는지 알았으니 결과값을 평가하는 법과 뉴럴 네트워크를 학습하는 법에 대해서 알아봅시다.

## Time for the Neural Network to Learn

포스팅 내용이 길어질거 같으니 커피 한잔이라도 드시고 오세요. 준비가 되셨나요? 이제 우리는 뉴런 네트워크의 결과가 어떻게 계산되는지 알았으니 학습에 대해서 알아볼 것입니다. **상위수준에서 바라본 뉴런 네트워크의 학습은 다른 데이터 과학 모델과 비슷합니다. cost 함수를 정의하고 최소화하기 위해 gradient descent optimization을 사용합니다.**

먼저 cost function을 최소화하기 위해서 우리가 당길 수 있는 레버가 무엇인지 생각해 봅시다. traditional linear, logistic regression 에서는 beta 계수 ( B0, B1, B2 등)를 찾아서 cost function을 최소화합니다. 신경 네트워크에서는 같은 일을 하지만 더욱 복잡하고 큰 규모로 수행합니다.

traditional regression에서 우리는 특정 beta를 독립시켜서 값을 변화시켜도 다른 계수에게 영향을 주지않게 할 수 있습니다. 따라서 각 beta 계수를 독립시키고 cost function에 주는 영향을 확인하면서 어떻게 해야 cost function을 최소화할 수 있는지 알아내는 것이죠.

<center>
<img src="https://miro.medium.com/max/649/1*BL2CSeVptZBBE6YoCfdyVg.jpeg"/>
</center>
Five feature logistic regression implemented via a neural network
{:.figure}

**신경 네트워크에서는 한 연결의 가중치를 바꾸게 되면 해당 뉴런 하위 계층에 모두 전달되게 됩니다. 따라서 각 뉴런은 작은 하나의 신경네트워크 처럼 동작하게 됩니다.** 예를들어 우리가 logistic regression 5개를 원하는 경우 위의 그림처럼 뉴런과 5개의 연결로 표현할 수 있습니다.

따라서 각 히든 레이의 신경 네트워크들은 결과물을 하위 계층에게 공급하는 모델 스택들이라고 할 수 있습니다.

## The Cost Function

이렇게 복잡도가 높다면 우리가 뭘 할 수 있을까요? 너무 낙심하지 마시고 차근차근 단계를 밟아봅시다. 먼저 우리의 목적을 확실하게 해봅시다. 학습 데이터가 주어지면 그것을 사용해 결과물을 뱉는 것입니다. 우리가 찾고자 하는 것은 가중치의 집합들이며 cost function을 최소화할 수 있는 상수들입니다. cost function은 우리의 예측이 실제 결과와 얼마나 연관성이 있는지 평가하는 것입니다.

신경 네트워크를 학습시키기 위해서 우리는 Mean Squared Error(MSE)를 cost function으로 사용할 것입니다.

**MSE = Sum [(Prediction - Actual)^2]*(1/num_observations)**


## Quick Review of Gradient Descent

<center>
<img src="https://miro.medium.com/max/768/1*IyYTmmRmGE_qaswQqCftUA.jpeg"/>
</center>
Illustration of Gradient Descent
{:.figure}

## Backpropagation

<center>
<img src="https://miro.medium.com/max/784/1*UY4-RIrSVgfuhAkawKIr2w.jpeg"/>
</center>
Forward propagation in a neural network
{:.figure}

<center>
<img src="https://miro.medium.com/max/796/1*0RIBu3Iz-aOOX9dyob_FHA.jpeg"/>
</center>
Backpropagation in a neural network
{:.figure}

## An Analogy that Helps - The Blame Game

<center>
<img src="https://miro.medium.com/max/411/1*7pfc5NsT3Y_5gNPWXNIqoQ.jpeg"/>
</center>
Neurons blame the most active upstream neurons
{:.figure}

## Trying it All Together

## Summary
*
