---
layout: post
title: Medium - Understanding Neural Networks
description: >
  <a href="https://towardsdatascience.com/understanding-neural-networks-19020b758230"> 원문 - Tony Yiu </a>
author: author
comments: true
---

Trend 파악을 위한 Medium 기고문 포스팅 - 뉴럴 네트워크에 대한 이해; 뉴럴 네트워크가 어떻게 동작하는지 알아보면서 딥러닝에 대해 알아봅시다.

<center>
<img src="https://miro.medium.com/max/2999/1*7wN5t9ILU0fpnhbMX2vtng.jpeg"/>
</center>

딥러닝은 요즘 아주 핫한 주제입니다. 그렇지만 무엇이 다른 머신러닝에 비해서 그렇게 특별한 것일까요? 이것은 꽤나 심도있는 질문이기 때문에 먼저 뉴럴 네트워크의 기본에 대해 배워봅시다. 인공신경망 네트워크는 딥러닝의 작업의 핵심입니다. 블랙박스 처럼 보이지만 실제로 깊게 파고 들어가보면 다른 모델처럼 예측을 달성하기 위한 것이죠. 이 포스트에서는 인공신경망 네트워크에 대해서 알아보고 결과적으로 어떻게 동작하는지, 뭘 하는건지 직관적으로 이해할 수 있게 되면 좋겠습니다.

## The 30,000 Feet View

먼저 우리가 다룰 수 있는 최상위 레벨에서 시작합니다. ** 신경 네트워크는 다수의 뉴런 네트워크로 우리가 예측을 하는데 사용하는 것입니다. ** 아래의 다어이그램은 5개의 입력을 받고 5개의 출력을 내뱉는 2개의 히든 레이어를 가진 신경 네트워크 입니다.

<center>
<img src="https://miro.medium.com/max/796/1*yGMk1GSKKbyKr_cMarlWnA.jpeg"/>
</center>
Neural network with two hidden layers
{:.figure}

먼저 왼쪽에서 부터 설명을 드리겠습니다.

1. 우리 모델의 입력값은 오랜지 색입니다.
1. 뉴런의 첫번째 히든 레이어는 파란색입니다.
1. 뉴런의 두번째 히든 레이어는 자주색입니다.
1. 우리 모델의 출력 결과는 초록색입니다.

점들을 잇는 선은 모든 뉴런이 어떻게 내부적으로 연결되어 있고 입력 레이어로부터 출력 레이어까지 어떤 경로를 거치는지 보여줍니다. 나중에 우리는 각 결과값에 대해서 단계별로 계산을 해볼 것입니다. 또한 신경 네트워크에서 회고로 알려진 실수에서 배우는 것을 어떻게 하는지 알아볼 것입니다.

## Getting our Bearings

먼저 우리의 방향성부터 확실히 합시다. 신경 네트워크는 정확히 뭘 하려는 것일까요? 다른 모델처럼 정확한 예측을 하려고 하는 것입니다. 입력값들과 짝을 이루는 결과 값들이 주어진다면 우리는 이러한 데이터 셋을 이용해서 입력값을 받으면 최대한 비슷한 결과값을 뱉어주려고 하는 것입니다.

위에서 그렸던 복잡한 그림은 잊어버리시고 지금은 아래의 간단한 신경 네트워크를 봅시다.

1. X (오렌지색) 는 우리의 단일 입력값으로 모델의 예측을 위해 주어집니다.
1. B1 (청록색)은 우리의 단일 회고 논리에서 예측에 사용될 매개변수 입니다.
<center>
<img src="https://miro.medium.com/max/701/1*aXK8cx57gGpTSStPSv5kqw.jpeg"/>
</center>
Logistic regression (with only one feature) implemented via a neural network
{:.figure}

이것은 신경 네트워크로 단일 회고 논리를 표현한 것입니다. 회고 논리 방정식을 다시 정의하고 싶다면 위의 그림에서 보였던 네트워크 요소와 방정식 요소의 색깔을 비교하시면 됩니다. 각 요소를 알아봅시다.


<center>
<img src="https://miro.medium.com/max/701/1*pbtFUSaW7UKrGgg-jbARxw.jpeg"/>
</center>
Logistic regression equation
{:.figure}

## Let's Add a Bit of Complexity Now

<center>
<img src="https://miro.medium.com/max/796/1*yGMk1GSKKbyKr_cMarlWnA.jpeg"/>
</center>
Our slightly more complicated neural network
{:.figure}

<center>
<img src="https://miro.medium.com/max/418/1*QKImlDHkRV-KkciOHxn-dw.jpeg"/>
</center>
The connections between Input 1 and Hidden Layer 1
{:.figure}

<center>
<img src="https://miro.medium.com/max/703/1*VxKto8Z35gqWFLFcf0wQ4g.jpeg"/>
</center>
Matrix math makes our life easier
{:.figure}

<center>
<img src="https://miro.medium.com/max/466/1*o3KBHNQsEXsYm0umpZiALg.jpeg"/>
</center>
Visualizing [W], [X], and [Z]
{:.figure}

## Time for the Neural Network to Learn

<center>
<img src="https://miro.medium.com/max/649/1*BL2CSeVptZBBE6YoCfdyVg.jpeg"/>
</center>
Five feature logistic regression implemented via a neural network
{:.figure}

<center>
<img src="https://miro.medium.com/max/768/1*IyYTmmRmGE_qaswQqCftUA.jpeg"/>
</center>
Illustration of Gradient Descent
{:.figure}

## Backpropagation

<center>
<img src="https://miro.medium.com/max/784/1*UY4-RIrSVgfuhAkawKIr2w.jpeg"/>
</center>
Forward propagation in a neural network
{:.figure}

<center>
<img src="https://miro.medium.com/max/796/1*0RIBu3Iz-aOOX9dyob_FHA.jpeg"/>
</center>
Backpropagation in a neural network
{:.figure}

## An Analogy that Helps - The Blame Game

<center>
<img src="https://miro.medium.com/max/411/1*7pfc5NsT3Y_5gNPWXNIqoQ.jpeg"/>
</center>
Neurons blame the most active upstream neurons
{:.figure}

## Trying it All Together

## Summary
*
